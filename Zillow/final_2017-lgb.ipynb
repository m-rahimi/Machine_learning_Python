{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took  23.156373024  seconds to read the dataframes\n"
     ]
    }
   ],
   "source": [
    "store = pd.HDFStore('../Data/store_final.h5')\n",
    "t1 = time.time()\n",
    "train = store[\"train\"]\n",
    "prop = store[\"prop\"]\n",
    "t2 = time.time()\n",
    "print 'it took ', t2-t1, ' seconds to read the dataframes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0309853730189 0.0326155099624\n",
      "0.197478559503 -0.184126738385\n"
     ]
    }
   ],
   "source": [
    "y = train.logerror\n",
    "mid = np.percentile(y, 50)\n",
    "y = y - mid\n",
    "q1 = np.percentile(y, 25)\n",
    "q3 = np.percentile(y, 75)\n",
    "print q1, q3\n",
    "interval = q3 - q1\n",
    "fac = 6.0\n",
    "interval = interval * fac / 2.\n",
    "hi = interval + mid\n",
    "lo = -interval + mid\n",
    "print hi, lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the x1 data frame:  (62852, 353)\n",
      "Size of the x0 data frame:  (14727, 353)\n",
      "Size of the properties data frame:  (62751, 353)\n",
      "2721 1660\n",
      "Size of the x1 data frame:  (58370, 353)\n",
      "Size of the x0 data frame:  (14727, 353)\n"
     ]
    }
   ],
   "source": [
    "# split the data to 9 months for train and 3 months for test\n",
    "x1 = train[(train.month < 8 ) & (train.year == 2017)]    # use for train\n",
    "x0 = train[(train.month > 7) & (train.year == 2017)]     # use for test\n",
    "\n",
    "print \"Size of the x1 data frame: \", x1.shape\n",
    "print \"Size of the x0 data frame: \", x0.shape\n",
    "\n",
    "# drop dublicate\n",
    "dup = x1[\"id_parcel\"].duplicated(keep='first')\n",
    "x1 = x1[~dup]\n",
    "print \"Size of the properties data frame: \", x1.shape\n",
    "\n",
    "\n",
    "y1 = x1.loc[~dup, 'logerror'].values\n",
    "y0 = x0['logerror'].values\n",
    "\n",
    "'''y1 = x1['logerror'].values\n",
    "y0 = x0['logerror'].values'''\n",
    "\n",
    "index_hi = y1 > hi   # drop 1480 points \n",
    "index_lo = y1 < lo    # drop 947 points\n",
    "print sum(index_hi), sum(index_lo)\n",
    "\n",
    "y1 = y1[(~index_lo) & (~index_hi)]\n",
    "x1 = x1[(~index_lo) & (~index_hi)]\n",
    "\n",
    "print \"Size of the x1 data frame: \", x1.shape\n",
    "print \"Size of the x0 data frame: \", x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cores 8\n"
     ]
    }
   ],
   "source": [
    "import lgb\n",
    "import multiprocessing\n",
    "\n",
    "ncpu = multiprocessing.cpu_count()\n",
    "print \"number of cores \" + str(ncpu)\n",
    "\n",
    "model = lgb.LgbReg(metric = 'l1', nthread = ncpu,\n",
    "                   max_depth = 50,\n",
    "                   max_bin = 10,\n",
    "                   learning_rate = 0.005,\n",
    "                   num_leaves = 240,\n",
    "                   bagging_fraction = 1.0,\n",
    "                   sub_feature = 0.6,\n",
    "                   min_data = 160,\n",
    "                   lambda_l1 = 0,\n",
    "                   lambda_l2 = 0,\n",
    "                   bagging_freq = 5,\n",
    "                   min_hessian = 0.01,\n",
    "                   verbos = -1)\n",
    "nround = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x1.drop([\"id_parcel\", \"month\", \"year\", \"logerror\"], axis=1), y1, num_iterations = nround) # Train the model without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on training data  0.0380272097593\n",
      "Error on 3 months test  0.0730595580531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print \"Error on training data \", mean_absolute_error(y1, model.predict(x1.drop([\"id_parcel\", \"month\", \"year\" , \"logerror\"], axis=1)))\n",
    "print \"Error on 3 months test \", mean_absolute_error(y0, model.predict(x0.drop([\"id_parcel\", \"month\", \"year\", \"logerror\"], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_2months = mean_absolute_error(y0, model.predict(x0.drop([\"id_parcel\", \"month\", \"year\", \"logerror\"], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0730595580531\n"
     ]
    }
   ],
   "source": [
    "print score_2months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove train duplicates\n",
    "duplicate = train[\"id_parcel\"].duplicated(keep='first')\n",
    "train = train[~duplicate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2904904, 350)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclude train from prop\n",
    "id_parcel = train[\"id_parcel\"].values\n",
    "prop = prop.set_index(\"id_parcel\")\n",
    "prop = prop.drop(id_parcel)\n",
    "prop = prop.reset_index()\n",
    "prop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0308762892166 0.0325720162774\n",
      "-0.183606628986 0.197083203978\n",
      "-2.05533164106 2.06880821605\n"
     ]
    }
   ],
   "source": [
    "y = train.logerror\n",
    "mid = np.percentile(y, 50)\n",
    "y = y - mid\n",
    "q1 = np.percentile(y, 25)\n",
    "q3 = np.percentile(y, 75)\n",
    "print q1, q3\n",
    "\n",
    "#fac = 8.0\n",
    "interval = q3 - q1\n",
    "interval = interval * fac / 2.\n",
    "hi_train = interval + mid\n",
    "lo_train = -interval + mid\n",
    "\n",
    "fac = 65.0\n",
    "interval = q3 - q1\n",
    "interval = interval * fac / 2.\n",
    "hi_test = interval + mid\n",
    "lo_test = -interval + mid\n",
    "\n",
    "print lo_train, hi_train\n",
    "print lo_test, hi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train data frame:  (77381, 350)\n",
      "Size of the prop  data frame:  (2904904, 350)\n",
      "Generate a list of outliers should be droped for training\n",
      "3381 1991\n",
      "Generate a list of outliers should be droped for testing\n",
      "102 35\n"
     ]
    }
   ],
   "source": [
    "y = train['logerror'].values\n",
    "x = train.drop(['month', 'year','logerror'], axis=1)\n",
    "print \"Size of the train data frame: \", x.shape\n",
    "print \"Size of the prop  data frame: \", prop.shape\n",
    "\n",
    "print(\"Generate a list of outliers should be droped for training\")\n",
    "index_hi = y > hi_train   \n",
    "index_lo = y < lo_train   \n",
    "print sum(index_hi), sum(index_lo)\n",
    "\n",
    "outliers_train = []\n",
    "for ii in range(y.shape[0]):\n",
    "    if index_hi[ii] or index_lo[ii]:\n",
    "        outliers_train.append(ii)\n",
    "        \n",
    "print(\"Generate a list of outliers should be droped for testing\")\n",
    "index_hi = y > hi_test   \n",
    "index_lo = y < lo_test   \n",
    "print sum(index_hi), sum(index_lo)\n",
    "\n",
    "outliers_test = []\n",
    "for ii in range(y.shape[0]):\n",
    "    if index_hi[ii] or index_lo[ii]:\n",
    "        outliers_test.append(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitDataFrameIntoSmaller(df, chunkSize = 100000): \n",
    "    listOfDf = list()\n",
    "    numberChunks = len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(i*chunkSize)\n",
    "    listOfDf.append(len(df))\n",
    "    return listOfDf\n",
    "\n",
    "split_index = splitDataFrameIntoSmaller(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score without outliers for the  1  fold is  0.0638002269689\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c6884e1c7a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_index_wo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutliers_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtest_index_wo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutliers_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "skf = KFold(n_splits = 10, shuffle = True, random_state = 44)\n",
    "\n",
    "train_pred = np.zeros(train.shape[0], dtype=np.float16)\n",
    "prop_pred = np.zeros(prop.shape[0], dtype=np.float16)\n",
    "scores1 = []; scores2 = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    \n",
    "    train_index_wo = [ix for ix in train_index if ix not in outliers_train]\n",
    "    test_index_wo = [ix for ix in test_index if ix not in outliers_test]\n",
    "     \n",
    "    x1, x0 = x.iloc[train_index_wo], x.iloc[test_index_wo]\n",
    "    y1, y0 = y[train_index_wo], y[test_index_wo]\n",
    "    \n",
    "    model.fit(x1.drop([\"id_parcel\"], axis=1), y1, num_iterations = nround) # Train the model without outliers\n",
    "    \n",
    "    #calculate score without second outliers\n",
    "    scores1.append(mean_absolute_error(y0, model.predict(x0.drop([\"id_parcel\"], axis=1))))\n",
    "    print \"Score without outliers for the \", len(scores1), \" fold is \", scores1[len(scores1)-1]\n",
    "    \n",
    "    #calculate score with outliers\n",
    "    x0 = x.iloc[test_index]\n",
    "    y0 = y[test_index]\n",
    "    \n",
    "    pred = model.predict(x0.drop([\"id_parcel\"], axis=1))\n",
    "    scores2.append(mean_absolute_error(y0, pred))\n",
    "#    print \"Score with outliers for the \", len(scores2), \" fold is \", scores2[len(scores2)-1]\n",
    "    \n",
    "    for ii, idx in enumerate(test_index):\n",
    "        train_pred[idx] = pred[ii]\n",
    "    \n",
    "    for ii in range(0, len(split_index)-1):\n",
    "        n1 = split_index[ii]; n2 = split_index[ii+1]\n",
    "        pred = model.predict(prop.iloc[n1:n2].drop(['id_parcel'], axis=1))\n",
    "        prop_pred[n1:n2] += pred\n",
    "    \n",
    "print \"Average score without outliers over all folds : \" , np.mean(scores1), \" \", np.std(scores1)\n",
    "print \"Average score with    outliers over all folds : \" , np.mean(scores2), \" \", np.std(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the missing\n"
     ]
    }
   ],
   "source": [
    "out = pd.DataFrame()\n",
    "out[\"ParcelId\"] = prop[\"id_parcel\"]\n",
    "months = [\"201610\"] #, \"201611\", \"201612\", \"201710\", \"201711\", \"201712\"]\n",
    "for col in months:\n",
    "    out[col] = map(lambda x: x/10.0, prop_pred)\n",
    "    \n",
    "out_train = pd.DataFrame()\n",
    "out_train[\"ParcelId\"] = train[\"id_parcel\"]\n",
    "for col in months:\n",
    "    out_train[col] = train_pred #+ 0.02 #IMPORTANT POINT: I add a constant to train prediction\n",
    "\n",
    "\n",
    "print(\"Read the missing\")\n",
    "miss = store[\"miss\"]\n",
    "\n",
    "med = train.logerror.median()\n",
    "for col in months:\n",
    "    miss[col] = med\n",
    "    \n",
    "miss = miss[[\"id_parcel\"]+months]\n",
    "miss.columns = [\"ParcelId\"] + months\n",
    "\n",
    "out = pd.concat([out, out_train, miss], axis=0)\n",
    "\n",
    "from datetime import datetime\n",
    "out.to_csv('test_2017.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,0.0641329423188\n"
     ]
    }
   ],
   "source": [
    "print str(score_2months)+\",\"+str(np.mean(scores1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
