{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute gini\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / np.float(ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_lr.out\n",
      "y_train_lreg.out\n",
      "y_train_lda.out\n",
      "y_train_knn.out\n",
      "y_train_nb.out\n",
      "y_train_rf.out\n",
      "y_train_ad.out\n",
      "y_train_gb.out\n",
      "y_train_xgb1.out\n",
      "y_train_xgb2.out\n",
      "y_train_xgb3.out\n",
      "y_train_xgb4.out\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.DataFrame()\n",
    "test_meta = pd.DataFrame()\n",
    "\n",
    "fold = \"Data/meta1/\"\n",
    "columns = [\"lr\", \"lreg\", \"lda\", \"knn\", \"nb\", \"rf\", \"ad\", \"gb\", \"xgb1\", \"xgb2\", \"xgb3\", \"xgb4\"]\n",
    "for col in columns:\n",
    "    name = \"y_train_\" + col + \".out\"\n",
    "    print name\n",
    "    train_meta[col] = np.loadtxt(fold+name)\n",
    "    name = \"y_test_\" + col + \".out\"\n",
    "#    test_meta[col] = np.loadtxt(fold+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv('Data/train.csv', na_values=\"-1\") # .iloc[0:200,:]\n",
    "#test_df = pd.read_csv('Data/test.csv', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_meta[\"y\"] = train_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.261480444372\n",
      "lreg 0.261289266811\n",
      "lda 0.261481382332\n",
      "knn 0.172214469131\n",
      "nb 0.236574751459\n",
      "rf 0.26888755436\n",
      "ad 0.271640830902\n",
      "gb 0.282853370441\n",
      "xgb1 0.287997209454\n",
      "xgb2 0.28460125291\n",
      "xgb3 0.248615122473\n",
      "xgb4 0.288576782051\n",
      "y 1.0\n"
     ]
    }
   ],
   "source": [
    "for col in train_meta.columns:\n",
    "    print col, eval_gini(train_meta[\"y\"], train_meta[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.289436475541\n"
     ]
    }
   ],
   "source": [
    "yy1 =  0.5*train_meta[\"xgb1\"] + 0.5*train_meta[\"xgb4\"] \n",
    "print eval_gini(train_meta[\"y\"], yy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.285001680513\n"
     ]
    }
   ],
   "source": [
    "yy2 =  0.4*train_meta[\"xgb2\"] + 0.6*train_meta[\"gb\"] \n",
    "print eval_gini(train_meta[\"y\"], yy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.285909338063\n"
     ]
    }
   ],
   "source": [
    "yy = 0.5*yy1 + 0.5*yy2\n",
    "print eval_gini(train_meta[\"y\"], yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_xgb5.out\n"
     ]
    }
   ],
   "source": [
    "name = \"y_train_\" + \"xgb5\" + \".out\"\n",
    "print name\n",
    "train_meta[col] = np.loadtxt(fold+\"y_train_xgb4.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_lr.out\n",
      "y_train_lreg.out\n",
      "y_train_lda.out\n",
      "y_train_knn.out\n",
      "y_train_nb.out\n",
      "y_train_rf.out\n",
      "y_train_ad.out\n",
      "y_train_gb.out\n",
      "y_train_xgb2.out\n",
      "y_train_xgb3.out\n",
      "y_train_xgb4.out\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.DataFrame()\n",
    "test_meta = pd.DataFrame()\n",
    "\n",
    "fold = \"/home/amin/Desktop/Porto/Data/meta1/\"\n",
    "columns = [\"lr\", \"lreg\", \"lda\", \"knn\", \"nb\", \"rf\", \"ad\", \"gb\",  \"xgb2\", \"xgb3\", \"xgb4\"]\n",
    "for col in columns:\n",
    "    name = \"y_train_\" + col + \".out\"\n",
    "    print name\n",
    "    train_meta[col] = np.loadtxt(fold+name)\n",
    "    name = \"y_test_\" + col + \".out\"\n",
    "    test_meta[col] = np.loadtxt(fold+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv('Data/train.csv', na_values=\"-1\") # .iloc[0:200,:]\n",
    "test_df = pd.read_csv('Data/test.csv', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from olivier\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1488028, 34)\n"
     ]
    }
   ],
   "source": [
    "target = train_df[\"target\"]\n",
    "ntrain = train_df.shape[0]\n",
    "data = pd.concat([train_df[train_features], test_df[train_features]], axis=0, ignore_index=True)\n",
    "print data.shape\n",
    "#del train_df, test_df\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8015609771\n",
      "3.0\n",
      "0.3741657387\n",
      "0.3734969879\n"
     ]
    }
   ],
   "source": [
    "# replace missing values with median\n",
    "columns = [\"ps_reg_03\", \"ps_car_11\", \"ps_car_12\", \"ps_car_14\"]\n",
    "data[columns] = data[columns].replace(-1, np.NaN)\n",
    "for col in columns:\n",
    "    med = data.loc[:ntrain-1, col].dropna().median()\n",
    "    data.loc[data[col].isnull(), col] = med\n",
    "    print med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "columns = [f for f in data.columns if \"_cat\" in f]\n",
    "\n",
    "for col in columns:\n",
    "    data.loc[data[col].isnull(), col] = -1\n",
    "    enc = LabelEncoder()\n",
    "    data[col] = enc.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshol = 500\n",
    "columns = [f for f in data.columns if \"_cat\" in f]\n",
    "\n",
    "for col in columns:\n",
    "    temp = data.loc[:ntrain-1, col]\n",
    "    density = temp.value_counts().sort_values()\n",
    "    count = density.iloc[0]\n",
    "    while count < threshol:\n",
    "        ix0 = density.index[0]; ix1 = density.index[1]\n",
    "        data.loc[data[col] == ix0, col] = ix1\n",
    "\n",
    "        temp = data.loc[:ntrain-1, col]\n",
    "        density = temp.value_counts().sort_values()\n",
    "        count = density.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('current feature %60s %4d in %5.1f', ('ps_reg_01_plus_ps_car_02_cat', 1, 1.6399224599202474e-05))\n",
      "('current feature %60s %4d in %5.1f', ('ps_reg_01_plus_ps_car_04_cat', 2, 0.04063146511713664))\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f', (name1, n_c + 1, (time.time() - start) / 60))\n",
    "\n",
    "    data[name1] = data[f1].apply(lambda x: str(x)) + \"_\" + data[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(data[name1].values))\n",
    "    data[name1] = lbl.transform(list(data[name1].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columns = [f for f in data.columns if \"_bin\" not in f and \"_cat\" not in f]\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "data[columns] = min_max_scaler.fit_transform(list(data[columns].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 48) (1488028, 48)\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([data.loc[:ntrain-1, :], train_meta], axis=1)\n",
    "test = pd.concat([data.loc[ntrain:, :], test_meta], axis=1)\n",
    "print X.shape, test.shape\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "params = {\n",
    "          'objective': 'binary:logistic',  \n",
    "          'max_depth': 4, \n",
    "          'learning_rate': 0.1,\n",
    "          'subsample': 0.8,  \n",
    "          'min_child_weight': 0.77, \n",
    "          'colsample_bytree': 0.9,\n",
    "#           'scale_pos_weight': 1.6,\n",
    "#           'gamma': 10,  \n",
    "#           'reg_alpha': 8,\n",
    "#           'reg_lambda': 1.3\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nFold ', 0)\n",
      "('  Best N trees = ', 56)\n",
      "('  Best gini = ', -0.275548)\n",
      "('  Gini = ', 0.2742348488403411)\n",
      "('\\nFold ', 1)\n",
      "('  Best N trees = ', 13)\n",
      "('  Best gini = ', -0.290107)\n",
      "('  Gini = ', 0.28785978801457635)\n",
      "('\\nFold ', 2)\n",
      "('  Best N trees = ', 30)\n",
      "('  Best gini = ', -0.288388)\n",
      "('  Gini = ', 0.2860452839382055)\n",
      "('\\nFold ', 3)\n",
      "('  Best N trees = ', 30)\n",
      "('  Best gini = ', -0.284483)\n",
      "('  Gini = ', 0.28261064503915667)\n",
      "('\\nFold ', 4)\n",
      "('  Best N trees = ', 35)\n",
      "('  Best gini = ', -0.289736)\n",
      "('  Gini = ', 0.2880261056887715)\n",
      "('\\nFold ', 5)\n",
      "('  Best N trees = ', 8)\n",
      "('  Best gini = ', -0.27732)\n",
      "('  Gini = ', 0.276899521472021)\n",
      "('\\nFold ', 6)\n",
      "('  Best N trees = ', 24)\n",
      "('  Best gini = ', -0.300253)\n",
      "('  Gini = ', 0.2983233095776787)\n",
      "('\\nFold ', 7)\n",
      "('  Best N trees = ', 53)\n",
      "('  Best gini = ', -0.284524)\n",
      "('  Gini = ', 0.283542779760792)\n",
      "('\\nFold ', 8)\n",
      "('  Best N trees = ', 18)\n",
      "('  Best gini = ', -0.295318)\n",
      "('  Gini = ', 0.2934663519481143)\n",
      "('\\nFold ', 9)\n",
      "('  Best N trees = ', 12)\n",
      "('  Best gini = ', -0.275939)\n",
      "('  Gini = ', 0.27441601251902825)\n",
      "\n",
      "Gini for full training set:\n",
      "0.284051582178\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = 0*target\n",
    "y_test_pred = 0\n",
    "\n",
    "# Set up folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "K = 10\n",
    "skf = StratifiedKFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(0)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, target)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = target.iloc[train_index].copy(), target.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "#    Enocode data\n",
    "#     for f in f_cats:\n",
    "#         X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "#                                                         trn_series=X_train[f],\n",
    "#                                                         val_series=X_valid[f],\n",
    "#                                                         tst_series=X_test[f],\n",
    "#                                                         target=y_train,\n",
    "#                                                         min_samples_leaf=200,\n",
    "#                                                         smoothing=10,\n",
    "#                                                         noise_level=0\n",
    "#                                                         )\n",
    "\n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid)\n",
    "    d_test = xgb.DMatrix(X_test)\n",
    "    \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    fit_model = xgb.train(params, d_train, 1000, \n",
    "                               watchlist,\n",
    "                               feval=gini_xgb,\n",
    "                               early_stopping_rounds=50,\n",
    "                               verbose_eval= False\n",
    "                             )\n",
    "    print( \"  Best N trees = \", fit_model.best_ntree_limit )\n",
    "    print( \"  Best gini = \", fit_model.best_score )\n",
    "        \n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict(d_valid)\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    # Accumulate test set predictions\n",
    "    y_test_pred += fit_model.predict(d_test)\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "print eval_gini(target, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "importance = fit_model.get_fscore()\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df['fscore'] = df['fscore'] / df['fscore'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14e5433e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2UAAAHXCAYAAAA86/IEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZkkSZIkddD/AYxtSf+cxw5ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14ff26d190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "df[-20:].plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 5))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cc = 1.0\n",
    "model = LogisticRegression(C=cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 11) (892816, 11)\n"
     ]
    }
   ],
   "source": [
    "X = train_meta.copy()\n",
    "test = test_meta.copy()\n",
    "print X.shape, test.shape\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nFold ', 0)\n",
      "('  Gini = ', 0.27214636366382905)\n",
      "('\\nFold ', 1)\n",
      "('  Gini = ', 0.28883348235779627)\n",
      "('\\nFold ', 2)\n",
      "('  Gini = ', 0.28615430427859845)\n",
      "('\\nFold ', 3)\n",
      "('  Gini = ', 0.28371952203322937)\n",
      "('\\nFold ', 4)\n",
      "('  Gini = ', 0.2873186098308499)\n",
      "('\\nFold ', 5)\n",
      "('  Gini = ', 0.27728778002157106)\n",
      "('\\nFold ', 6)\n",
      "('  Gini = ', 0.2968527857474561)\n",
      "('\\nFold ', 7)\n",
      "('  Gini = ', 0.2849901839672515)\n",
      "('\\nFold ', 8)\n",
      "('  Gini = ', 0.29118833794974186)\n",
      "('\\nFold ', 9)\n",
      "('  Gini = ', 0.2726991173929736)\n",
      "\n",
      "Gini for full training set:\n",
      "0.28411342804\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = 0*target\n",
    "y_test_pred = 0\n",
    "\n",
    "# Set up folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "K = 10\n",
    "skf = StratifiedKFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(0)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, target)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = target.iloc[train_index].copy(), target.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "# #    Enocode data\n",
    "#     for f in f_cats:\n",
    "#         X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "#                                                         trn_series=X_train[f],\n",
    "#                                                         val_series=X_valid[f],\n",
    "#                                                         tst_series=X_test[f],\n",
    "#                                                         target=y_train,\n",
    "#                                                         min_samples_leaf=200,\n",
    "#                                                         smoothing=10,\n",
    "#                                                         noise_level=0\n",
    "#                                                         )\n",
    "\n",
    "    eval_set=[(X_valid,y_valid)]\n",
    "    fit_model = model.fit( X_train, y_train) \n",
    "#     , \n",
    "#                                eval_set=eval_set,\n",
    "#                                eval_metric=gini_xgb,\n",
    "#                                early_stopping_rounds=50,\n",
    "#                                verbose=False\n",
    "#                              )\n",
    "#    print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "#    print( \"  Best gini = \", model.best_score )\n",
    "        \n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    # Accumulate test set predictions\n",
    "    y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "print eval_gini(target, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
