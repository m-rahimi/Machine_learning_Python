{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_count( df, group_cols, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Aggregating by \", group_cols , '...' )\n",
    "    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "\n",
    "def do_countuniq( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Counting unqiue \", counted, \" by \", group_cols , '...' )\n",
    "    # print('the Id of train_df while function before merge: ',id(df)) # the same with train_df\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    # print('the Id of train_df while function after merge: ',id(df)) # id changes\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "    \n",
    "def do_cumcount( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Cumulative count by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount()\n",
    "    df[agg_name]=gp.values\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "\n",
    "def do_mean( df, group_cols, counted, agg_name, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Calculating mean of \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "\n",
    "def do_var( df, group_cols, counted, agg_name, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Calculating variance of \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_modelfit_nocv(dtrain, dvalid, predictors, target='target', feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None,metrics='auc'):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric':'auc',\n",
    "        'learning_rate': 0.2, # 【consider use 0.1】\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'scale_pos_weight': 200, # because training data is extremely unbalanced\n",
    "        'num_leaves': 7,  # we should let it be smaller than 2^(max_depth), default=31\n",
    "        'max_depth': 3,  # -1 means no limit, default=-1\n",
    "        'min_data_per_leaf': 100,  # alias=min_data_per_leaf , min_data, min_child_samples, default=20\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values,default=255\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.default=1.0, alias=bagging_fraction\n",
    "        'subsample_freq': 1,  # k means will perform bagging at every k iteration, <=0 means no enable,alias=bagging_freq,default=0\n",
    "        'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.alias:feature_fraction\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf),default=1e-3,Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 20, # should be equal to REAL cores:http://xgboost.readthedocs.io/en/latest/how_to/external_memory.html\n",
    "        'verbose': 0\n",
    "        # 'random_state':666 [LightGBM] [Warning] Unknown parameter: random_state\n",
    "        # 'feature_fraction_seed': 666,\n",
    "        # 'bagging_seed': 666, # alias=bagging_fraction_seed\n",
    "        # 'data_random_seed': 666 # random seed for data partition in parallel learning (not include feature parallel)\n",
    "    }\n",
    "    # lgb_params.update(params) # Python dict.update()\n",
    "\n",
    "    print(\"load train_df into lgb.Dataset...\")\n",
    "    # free_raw_data (bool, optional (default=True)) – If True, raw data is freed after constructing inner Dataset.\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    del dtrain\n",
    "    print(\"load valid_df into lgb.Dataset...\")\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    del dvalid\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgvalid], \n",
    "                     valid_names=['valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "    \n",
    "    del xgtrain, xgvalid\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "    gc.collect()\n",
    "\n",
    "    return (bst1,bst1.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=184903891-1 # the first line is columns' name\n",
    "nchunk=nrows #75000000 # 【The more the better,75000000】\n",
    "val_size=2500000 # 1/10 of 'nchunk'\n",
    "frm=0 #nrows-75000000\n",
    "\n",
    "debug = False\n",
    "    \n",
    "to=frm+nchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data... 0 184903890\n",
      "loading test data...\n",
      "Extracting new features...\n",
      "Counting unqiue  channel  by  ['ip'] ...\n",
      "X0 max value =  165\n",
      "Cumulative count by  ['ip', 'device', 'os'] ...\n",
      "X1 max value =  282426\n",
      "Counting unqiue  hour  by  ['ip', 'day'] ...\n",
      "X2 max value =  24\n",
      "Counting unqiue  app  by  ['ip'] ...\n",
      "X3 max value =  277\n",
      "Counting unqiue  os  by  ['ip', 'app'] ...\n",
      "X4 max value =  148\n",
      "Counting unqiue  device  by  ['ip'] ...\n",
      "X5 max value =  551\n",
      "Counting unqiue  channel  by  ['app'] ...\n",
      "X6 max value =  49\n",
      "Cumulative count by  ['ip'] ...\n",
      "X7 max value =  1421255\n",
      "Counting unqiue  app  by  ['ip', 'device', 'os'] ...\n",
      "X8 max value =  100\n",
      "Aggregating by  ['ip', 'day', 'hour'] ...\n",
      "ip_tcount max value =  44259\n",
      "Aggregating by  ['ip', 'app'] ...\n",
      "ip_app_count max value =  220743\n",
      "Aggregating by  ['ip', 'app', 'os'] ...\n",
      "ip_app_os_count max value =  55159\n",
      "Calculating variance of  hour  by  ['ip', 'day', 'channel'] ...\n",
      "ip_tchan_count max value =  264.5\n",
      "Calculating variance of  hour  by  ['ip', 'app', 'os'] ...\n",
      "ip_app_os_var max value =  264.5\n",
      "Calculating variance of  day  by  ['ip', 'app', 'channel'] ...\n",
      "ip_app_channel_var_day max value =  8.0\n",
      "Calculating mean of  hour  by  ['ip', 'app', 'channel'] ...\n",
      "ip_app_channel_mean_hour max value =  23.0\n",
      "doing nextClick 2...\n",
      "0      5340.0\n",
      "1      5547.0\n",
      "2      5925.0\n",
      "3      5110.0\n",
      "4     33756.0\n",
      "5     40007.0\n",
      "6     46131.0\n",
      "7     17250.0\n",
      "8     12685.0\n",
      "9         NaN\n",
      "10     6052.0\n",
      "11     6738.0\n",
      "12     8115.0\n",
      "13    54732.0\n",
      "14    43188.0\n",
      "15    29699.0\n",
      "16    36332.0\n",
      "17    37574.0\n",
      "18     4881.0\n",
      "19    23438.0\n",
      "20    42326.0\n",
      "21     5271.0\n",
      "22     6642.0\n",
      "23     4542.0\n",
      "24    29815.0\n",
      "25    52108.0\n",
      "26    30433.0\n",
      "27     3648.0\n",
      "28     8690.0\n",
      "29    38181.0\n",
      "Name: nextClick, dtype: float32\n",
      "vars and data type: \n",
      "predictors ['nextClick', 'app', 'device', 'os', 'channel', 'hour', 'day', 'ip_tcount', 'ip_tchan_count', 'ip_app_count', 'ip_app_os_count', 'ip_app_os_var', 'ip_app_channel_var_day', 'ip_app_channel_mean_hour', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/Software/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  182403890\n",
      "valid size:  2500000\n",
      "test size :  18790469\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 182403890 entries, 0 to 182403889\n",
      "Data columns (total 25 columns):\n",
      "app                         uint16\n",
      "channel                     uint16\n",
      "device                      uint16\n",
      "ip                          uint32\n",
      "is_attributed               uint8\n",
      "os                          uint16\n",
      "hour                        uint8\n",
      "day                         uint8\n",
      "X0                          uint8\n",
      "X1                          uint32\n",
      "X2                          uint8\n",
      "X3                          uint8\n",
      "X4                          uint8\n",
      "X5                          uint16\n",
      "X6                          uint32\n",
      "X7                          uint32\n",
      "X8                          uint32\n",
      "ip_tcount                   uint16\n",
      "ip_app_count                uint16\n",
      "ip_app_os_count             uint16\n",
      "ip_tchan_count              float32\n",
      "ip_app_os_var               float32\n",
      "ip_app_channel_var_day      float32\n",
      "ip_app_channel_mean_hour    float32\n",
      "nextClick                   float32\n",
      "dtypes: float32(5), uint16(8), uint32(5), uint8(7)\n",
      "memory usage: 12.1 GB\n",
      "Training...\n",
      "load train_df into lgb.Dataset...\n",
      "load valid_df into lgb.Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/Software/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/amin/Software/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[10]\tvalid's auc: 0.978148\n",
      "[20]\tvalid's auc: 0.981146\n",
      "[30]\tvalid's auc: 0.984256\n",
      "[40]\tvalid's auc: 0.98662\n",
      "[50]\tvalid's auc: 0.987196\n",
      "[60]\tvalid's auc: 0.988168\n",
      "[70]\tvalid's auc: 0.988426\n",
      "[80]\tvalid's auc: 0.988692\n",
      "[90]\tvalid's auc: 0.988933\n",
      "[100]\tvalid's auc: 0.989229\n",
      "[110]\tvalid's auc: 0.989422\n",
      "[120]\tvalid's auc: 0.989583\n",
      "[130]\tvalid's auc: 0.989608\n",
      "[140]\tvalid's auc: 0.989817\n",
      "[150]\tvalid's auc: 0.989804\n",
      "[160]\tvalid's auc: 0.989838\n",
      "[170]\tvalid's auc: 0.989917\n",
      "[180]\tvalid's auc: 0.989966\n",
      "[190]\tvalid's auc: 0.99013\n",
      "[200]\tvalid's auc: 0.990181\n",
      "[210]\tvalid's auc: 0.990211\n",
      "[220]\tvalid's auc: 0.990241\n",
      "[230]\tvalid's auc: 0.990244\n",
      "[240]\tvalid's auc: 0.990333\n",
      "[250]\tvalid's auc: 0.990328\n",
      "[260]\tvalid's auc: 0.990327\n",
      "[270]\tvalid's auc: 0.990374\n",
      "[280]\tvalid's auc: 0.99042\n",
      "[290]\tvalid's auc: 0.990424\n",
      "[300]\tvalid's auc: 0.990424\n",
      "[310]\tvalid's auc: 0.990475\n",
      "[320]\tvalid's auc: 0.990477\n",
      "[330]\tvalid's auc: 0.990487\n",
      "[340]\tvalid's auc: 0.990532\n",
      "[350]\tvalid's auc: 0.990527\n",
      "[360]\tvalid's auc: 0.990542\n",
      "[370]\tvalid's auc: 0.990505\n",
      "[380]\tvalid's auc: 0.990515\n",
      "[390]\tvalid's auc: 0.990529\n",
      "Early stopping, best iteration is:\n",
      "[360]\tvalid's auc: 0.990542\n",
      "\n",
      "Model Report\n",
      "bst1.best_iteration:  360\n",
      "auc: 0.9905422104041749\n",
      "[1445.940101146698]: model training time\n"
     ]
    }
   ],
   "source": [
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8', # 【consider bool?need test】\n",
    "        'click_id'      : 'uint32', # 【consider 'attributed_time'?】\n",
    "        }\n",
    "\n",
    "print('loading train data...',frm,to)\n",
    "# usecols:Using this parameter results in much faster parsing time and lower memory usage.\n",
    "train_df = pd.read_csv(\"../../Data/train.csv\", parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "\n",
    "print('loading test data...')\n",
    "if debug:\n",
    "    test_df = pd.read_csv(\"../../Data/test.csv\", nrows=100000, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "else:\n",
    "    test_df = pd.read_csv(\"../../Data/test.csv\", parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "len_train = len(train_df)\n",
    "# 【consider using concat more effiencent?add two more useless columns?】\n",
    "train_df=train_df.append(test_df) # Shouldn't process individually,because of lots of count,mean,var variables\n",
    "# train_df['is_attributed'] = train_df['is_attributed'].fillna(-1)\n",
    "train_df['is_attributed'].fillna(-1,inplace=True)\n",
    "train_df['is_attributed'] = train_df['is_attributed'].astype('uint8',copy=False)\n",
    "# train_df['click_id'] = train_df['click_id'].fillna(-1)\n",
    "train_df['click_id'].fillna(-1,inplace=True)\n",
    "train_df['click_id'] = train_df['click_id'].astype('uint32',copy=False)\n",
    "\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "print('Extracting new features...')\n",
    "train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "gc.collect()\n",
    "\n",
    "# 【Not same with the original kernel?】\n",
    "# print('the Id of train_df before function: ',id(train_df))\n",
    "train_df = do_countuniq( train_df, ['ip'], 'channel', 'X0', 'uint8', show_max=True ); gc.collect()\n",
    "# print('the Id of train_df after function: ',id(train_df)) # the same id with 'df' returned\n",
    "train_df = do_cumcount( train_df, ['ip', 'device', 'os'], 'app', 'X1', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip', 'day'], 'hour', 'X2', 'uint8', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip'], 'app', 'X3', 'uint8', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip', 'app'], 'os', 'X4', 'uint8', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip'], 'device', 'X5', 'uint16', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['app'], 'channel', 'X6', show_max=True ); gc.collect()\n",
    "train_df = do_cumcount( train_df, ['ip'], 'os', 'X7', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip', 'device', 'os'], 'app', 'X8', show_max=True ); gc.collect()\n",
    "train_df = do_count( train_df, ['ip', 'day', 'hour'], 'ip_tcount', show_max=True ); gc.collect()\n",
    "train_df = do_count( train_df, ['ip', 'app'], 'ip_app_count', show_max=True ); gc.collect()\n",
    "train_df = do_count( train_df, ['ip', 'app', 'os'], 'ip_app_os_count', 'uint16', show_max=True ); gc.collect()\n",
    "train_df = do_var( train_df, ['ip', 'day', 'channel'], 'hour', 'ip_tchan_count', show_max=True ); gc.collect()\n",
    "train_df = do_var( train_df, ['ip', 'app', 'os'], 'hour', 'ip_app_os_var', show_max=True ); gc.collect()\n",
    "train_df = do_var( train_df, ['ip', 'app', 'channel'], 'day', 'ip_app_channel_var_day', show_max=True ); gc.collect()\n",
    "train_df = do_mean( train_df, ['ip', 'app', 'channel'], 'hour', 'ip_app_channel_mean_hour', show_max=True ); gc.collect()\n",
    "train_df['ip_tcount'] = train_df['ip_tcount'].astype('uint16',copy=False)\n",
    "train_df['ip_app_count'] = train_df['ip_app_count'].astype('uint16',copy=False)\n",
    "train_df['ip_app_os_count'] = train_df['ip_app_os_count'].astype('uint16',copy=False)\n",
    "\n",
    "# nextclick----------------------------------------------------------------------------------------------------------\n",
    "# print('doing nextClick')\n",
    "# predictors=[]\n",
    "# new_feature = 'nextClick'\n",
    "# filename='nextClick_%d_%d.csv'%(frm,to)\n",
    "\n",
    "# if os.path.exists(filename):\n",
    "#     print('loading from save file')\n",
    "#     QQ=pd.read_csv(filename).values\n",
    "# else:\n",
    "#     D=2**26\n",
    "#     train_df['category'] = (train_df['ip'].astype(str) + \"_\" + train_df['app'].astype(str) + \"_\" + train_df['device'].astype(str) \\\n",
    "#         + \"_\" + train_df['os'].astype(str)).apply(hash) % D\n",
    "#     # from 1970/1/1, 50year*365day*24*60*60=1,576,800,000 seconds, so 2,000,000,000 is enough\n",
    "#     click_buffer= np.full(D, 3000000000, dtype=np.uint32) # Return a new array of given shape and type, filled with fill_value.\n",
    "\n",
    "#     train_df['epochtime']= train_df['click_time'].astype(np.int64,copy=False) // 10 ** 9\n",
    "#     next_clicks= []\n",
    "#     # After reverse, the time becomes future to past, make next_clicks positive\n",
    "#     for category, t in zip(reversed(train_df['category'].values), reversed(train_df['epochtime'].values)):\n",
    "#         next_clicks.append(click_buffer[category]-t)\n",
    "#         click_buffer[category]= t\n",
    "#     del(click_buffer)\n",
    "#     QQ= list(reversed(next_clicks))\n",
    "\n",
    "#     if not debug:\n",
    "#         print('saving')\n",
    "#         pd.DataFrame(QQ).to_csv(filename,index=False)\n",
    "\n",
    "# train_df.drop(['epochtime','category','click_time'], axis=1, inplace=True)\n",
    "\n",
    "# train_df[new_feature] = pd.Series(QQ).astype('float32',copy=False)\n",
    "# predictors.append(new_feature)\n",
    "# train_df[new_feature+'_shift'] = train_df[new_feature].shift(+1).values\n",
    "# predictors.append(new_feature+'_shift')\n",
    "\n",
    "# del QQ\n",
    "# gc.collect()\n",
    "\n",
    "#=====================================================================================================\n",
    "print('doing nextClick 2...')\n",
    "predictors=[]\n",
    "\n",
    "train_df['click_time'] = (train_df['click_time'].astype(np.int64,copy=False) // 10 ** 9).astype(np.int32,copy=False)\n",
    "train_df['nextClick'] = (train_df.groupby(['ip', 'app', 'device', 'os']).click_time.shift(-1) - train_df.click_time).astype(np.float32,copy=False)\n",
    "print(train_df['nextClick'].head(30))\n",
    "train_df.drop(['click_time'], axis=1, inplace=True)\n",
    "predictors.append('nextClick')\n",
    "gc.collect()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "print(\"vars and data type: \")\n",
    "target = 'is_attributed'\n",
    "predictors.extend(['app','device','os', 'channel', 'hour', 'day', \n",
    "              'ip_tcount', 'ip_tchan_count', 'ip_app_count',\n",
    "              'ip_app_os_count', 'ip_app_os_var',\n",
    "              'ip_app_channel_var_day','ip_app_channel_mean_hour',\n",
    "              'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8'])\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day'] # 【consider delete 'day' and others】\n",
    "print('predictors',predictors)\n",
    "\n",
    "test_df = train_df[len_train:]\n",
    "test_df.drop(columns='is_attributed',inplace=True)\n",
    "train_df.drop(columns='click_id',inplace=True)\n",
    "val_df = train_df[(len_train-val_size):len_train] # Validation set\n",
    "train_df = train_df[:(len_train-val_size)]\n",
    "\n",
    "print(\"train size: \", len(train_df))\n",
    "print(\"valid size: \", len(val_df))\n",
    "print(\"test size : \", len(test_df))\n",
    "train_df.info()\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id']\n",
    "gc.collect()\n",
    "\n",
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "(bst,best_iteration) = lgb_modelfit_nocv(\n",
    "                        train_df, \n",
    "                        val_df, \n",
    "                        predictors, \n",
    "                        target, \n",
    "                        early_stopping_rounds=30, \n",
    "                        verbose_eval=True, \n",
    "                        num_boost_round=1000, \n",
    "                        categorical_features=categorical)\n",
    "#del train_df\n",
    "#del val_df\n",
    "gc.collect()\n",
    "print('[{}]: model training time'.format(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('sub_it%d.csv'%(3),index=False,float_format='%.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = pd.HDFStore('../../store/storage4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf[\"train\"] = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf[\"valid\"] = val_df\n",
    "hdf[\"test\"] = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
