{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncpu = 8 # It should be modified if run of midawy :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: ../Data/store1.h5\n",
      "/miss             frame        (shape->[11437,3])    \n",
      "/prop             frame        (shape->[2883630,254])\n",
      "/train            frame        (shape->[90275,256])  \n"
     ]
    }
   ],
   "source": [
    "store = pd.HDFStore('../Data/store1.h5')\n",
    "print store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = store[\"train\"]\n",
    "prop = store[\"prop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prop shape (2883630, 236)\n",
      "train shape (90275, 238)\n"
     ]
    }
   ],
   "source": [
    "dd = ['type_architectural',\n",
    " 'area_basement',\n",
    " 'num_bathroom',\n",
    " 'num_bedroom',\n",
    " 'type_framing',\n",
    " 'num_bathroom_calc',\n",
    " 'type_deck',\n",
    " 'area_liveperi_finished',\n",
    " 'num_bath',\n",
    " 'pooltypeid10',\n",
    " 'region_county',\n",
    " 'type_story',\n",
    " 'type_material',\n",
    " 'area_shed',\n",
    " 'tax_year',\n",
    " 'num_rot75_X',\n",
    " 'num_rot75_Y']\n",
    "\n",
    "train = train.drop(dd + [\"area_total_calc\"], axis=1)\n",
    "prop = prop.drop(dd + [\"area_total_calc\"], axis=1)\n",
    "\n",
    "print \"prop shape \" + str(prop.shape)\n",
    "print \"train shape \" + str(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0253 0.0392\n",
      "0.264 -0.252\n"
     ]
    }
   ],
   "source": [
    "y = train.logerror\n",
    "mid = np.percentile(y, 50)\n",
    "y = y - mid\n",
    "q1 = np.percentile(y, 25)\n",
    "q3 = np.percentile(y, 75)\n",
    "print q1, q3\n",
    "interval = q3 - q1\n",
    "fac = 8.0\n",
    "interval = interval * fac / 2.\n",
    "hi = interval + mid\n",
    "lo = -interval + mid\n",
    "print hi, lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the x1 data frame:  (81733, 238)\n",
      "Size of the x0 data frame:  (8542, 238)\n",
      "2088 1432\n",
      "Size of the x1 data frame:  (78213, 238)\n",
      "Size of the x0 data frame:  (8542, 238)\n"
     ]
    }
   ],
   "source": [
    "# split the data to 9 months for train and 3 months for test\n",
    "x1 = train[train.month < 10]    # use for train\n",
    "x0 = train[train.month > 9]     # use for test\n",
    "print \"Size of the x1 data frame: \", x1.shape\n",
    "print \"Size of the x0 data frame: \", x0.shape\n",
    "\n",
    "y1 = x1['logerror'].values\n",
    "y0 = x0['logerror'].values\n",
    "\n",
    "index_hi = y1 > hi   # drop 1480 points\n",
    "index_lo = y1 < lo    # drop 947 points\n",
    "print sum(index_hi), sum(index_lo)\n",
    "\n",
    "y1 = y1[(~index_lo) & (~index_hi)]\n",
    "x1 = x1[(~index_lo) & (~index_hi)]\n",
    "\n",
    "print \"Size of the x1 data frame: \", x1.shape\n",
    "print \"Size of the x0 data frame: \", x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/Software/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgb\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [\"rate\", \"depth\", \"nround\", \"sample\", \"colsample\", \"child\", \"reg_alpha\", \"reg_lambda\", \"score\"]\n",
    "result = pd.DataFrame(columns=col)\n",
    "\n",
    "for rate in [0.01]:\n",
    "    for depth in [9]:\n",
    "        for nround in [470, 475, 480]:\n",
    "            for sample in [1.0]:\n",
    "                for colsample in [0.5]:\n",
    "                    for child in [65, 70, 75]:\n",
    "                        for L1 in [0]:\n",
    "                            for L2 in [1]:\n",
    "                                model = xgb.XGBoostReg(eval_metric = 'mae', nthread = ncpu, silent = 1,\n",
    "                                                   min_child_weight = child,\n",
    "                                                   subsample = sample,\n",
    "                                                   colsample_bytree = colsample,\n",
    "                                                   eta = rate,\n",
    "                                                   max_depth = depth,\n",
    "                                                   reg_lambda = L2,\n",
    "                                                   reg_alpha = L1)\n",
    "\n",
    "                                model.fit(x1.drop([\"id_parcel\", \"month\", \"logerror\"], axis=1), y1, num_boost_round= nround)\n",
    "\n",
    "                                score = mean_absolute_error(y0, model.predict(x0.drop([\"id_parcel\", \"month\", \"logerror\"], axis=1)))\n",
    "\n",
    "                                result = result.append(pd.DataFrame([[rate, depth, nround, sample, colsample,\n",
    "                                                                      child, L1, L2, score]], columns=col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = result.sort_values(\"score\")\n",
    "result.to_csv(\"result.csv\")\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run best\n",
    "model = xgb.XGBoostReg(\n",
    "        eval_metric = 'mae',\n",
    "        nthread = ncpu,\n",
    "        eta = result.iloc[0].rate,\n",
    "        max_depth = np.int(result.iloc[0].depth),\n",
    "        subsample = result.iloc[0].subsample,\n",
    "        colsample_bytree = result.iloc[0].colsample,\n",
    "        min_child_weight = result.iloc[0].child,\n",
    "        silent = 1\n",
    "        )\n",
    "nround = np.int(result.iloc[0].nround)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on training data  0.0547418632759\n",
      "Error on 3 months test  0.0716829992608\n"
     ]
    }
   ],
   "source": [
    "model.fit(x1.drop([\"id_parcel\", \"month\", \"logerror\"], axis=1), y1, num_boost_round= nround) # Train the model without outliers\n",
    "\n",
    "print \"Error on training data \", mean_absolute_error(y1, model.predict(x1.drop([\"id_parcel\", \"month\", \"logerror\"], axis=1)))\n",
    "print \"Error on 3 months test \", mean_absolute_error(y0, model.predict(x0.drop([\"id_parcel\", \"month\", \"logerror\"], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "importance = model.get_score()\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "df.to_csv(\"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train data frame:  (90150, 238)\n",
      "Size of the prop data frame:  (2883630, 236)\n"
     ]
    }
   ],
   "source": [
    "# we have duplicate in train :(\n",
    "# we can have three simple strategies\n",
    "# 1- keep first one; 2- keep last one; 3- average\n",
    "# I think the logerror reduce from first to last but I am not sure\n",
    "# it is a very important point\n",
    "duplicate = train[\"id_parcel\"].duplicated(keep='first')\n",
    "train = train[~duplicate]\n",
    "print \"Size of the train data frame: \", train.shape\n",
    "print \"Size of the prop data frame: \", prop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0313 0.0332\n",
      "-0.252 0.264\n",
      "-2.09025 2.10225\n"
     ]
    }
   ],
   "source": [
    "y = train.logerror\n",
    "mid = np.percentile(y, 50)\n",
    "y = y - mid\n",
    "q1 = np.percentile(y, 25)\n",
    "q3 = np.percentile(y, 75)\n",
    "print q1, q3\n",
    "\n",
    "fac = 8.0\n",
    "interval = q3 - q1\n",
    "interval = interval * fac / 2.\n",
    "hi_train = interval + mid\n",
    "lo_train = -interval + mid\n",
    "\n",
    "fac = 65.0\n",
    "interval = q3 - q1\n",
    "interval = interval * fac / 2.\n",
    "hi_test = interval + mid\n",
    "lo_test = -interval + mid\n",
    "\n",
    "print lo_train, hi_train\n",
    "print lo_test, hi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train data frame:  (90150, 236)\n",
      "Size of the prop  data frame:  (2883630, 236)\n",
      "Generate a list of outliers should be droped for training\n",
      "2310 1568\n",
      "Generate a list of outliers should be droped for testing\n",
      "51 46\n"
     ]
    }
   ],
   "source": [
    "y = train['logerror'].values\n",
    "x = train.drop(['month', 'logerror'], axis=1)\n",
    "print \"Size of the train data frame: \", x.shape\n",
    "print \"Size of the prop  data frame: \", prop.shape\n",
    "\n",
    "print(\"Generate a list of outliers should be droped for training\")\n",
    "index_hi = y > hi_train\n",
    "index_lo = y < lo_train\n",
    "print sum(index_hi), sum(index_lo)\n",
    "\n",
    "outliers_train = []\n",
    "for ii in range(y.shape[0]):\n",
    "    if index_hi[ii] or index_lo[ii]:\n",
    "        outliers_train.append(ii)\n",
    "\n",
    "print(\"Generate a list of outliers should be droped for testing\")\n",
    "index_hi = y > hi_test\n",
    "index_lo = y < lo_test\n",
    "print sum(index_hi), sum(index_lo)\n",
    "\n",
    "outliers_test = []\n",
    "for ii in range(y.shape[0]):\n",
    "    if index_hi[ii] or index_lo[ii]:\n",
    "        outliers_test.append(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitDataFrameIntoSmaller(df, chunkSize = 100000):\n",
    "    listOfDf = list()\n",
    "    numberChunks = len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(i*chunkSize)\n",
    "    listOfDf.append(len(df))\n",
    "    return listOfDf\n",
    "\n",
    "split_index = splitDataFrameIntoSmaller(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/Software/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score without outliers for the  1  fold is  0.445987432759\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-417bcbad942a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_index_wo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutliers_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtest_index_wo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutliers_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 44)\n",
    "\n",
    "train_pred = np.zeros(train.shape[0], dtype=np.float16)\n",
    "prop_pred = np.zeros(prop.shape[0], dtype=np.float16)\n",
    "scores1 = []; scores2 = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "\n",
    "    train_index_wo = [ix for ix in train_index if ix not in outliers_train]\n",
    "    test_index_wo = [ix for ix in test_index if ix not in outliers_test]\n",
    "\n",
    "    x1, x0 = x.iloc[train_index_wo], x.iloc[test_index_wo]\n",
    "    y1, y0 = y[train_index_wo], y[test_index_wo]\n",
    "\n",
    "    model.fit(x1.drop([\"id_parcel\"], axis=1), y1) # Train the model without outliers\n",
    "\n",
    "    #calculate score without second outliers\n",
    "    scores1.append(mean_absolute_error(y0, model.predict(x0.drop([\"id_parcel\"], axis=1))))\n",
    "    print \"Score without outliers for the \", len(scores1), \" fold is \", scores1[len(scores1)-1]\n",
    "\n",
    "    #calculate score with outliers\n",
    "    x0 = x.iloc[test_index]\n",
    "    y0 = y[test_index]\n",
    "\n",
    "    pred = model.predict(x0.drop([\"id_parcel\"], axis=1))\n",
    "    scores2.append(mean_absolute_error(y0, pred))\n",
    "#    print \"Score with outliers for the \", len(scores2), \" fold is \", scores2[len(scores2)-1]\n",
    "\n",
    "    for ii, idx in enumerate(test_index):\n",
    "        train_pred[idx] = pred[ii]\n",
    "\n",
    "    for ii in range(0, len(split_index)-1):\n",
    "        n1 = split_index[ii]; n2 = split_index[ii+1]\n",
    "        pred = model.predict(prop.iloc[n1:n2].drop(['id_parcel'], axis=1))\n",
    "        prop_pred[n1:n2] += pred\n",
    "\n",
    "print \"Average score without outliers over all folds : \" , np.mean(scores1), \" \", np.std(scores1)\n",
    "print \"Average score with    outliers over all folds : \" , np.mean(scores2), \" \", np.std(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame()\n",
    "out[\"ParcelId\"] = prop[\"id_parcel\"]\n",
    "months = [\"201610\", \"201611\", \"201612\", \"201710\", \"201711\", \"201712\"]\n",
    "for col in months:\n",
    "    out[col] = map(lambda x: x/10.0, prop_pred)\n",
    "    \n",
    "out_train = pd.DataFrame()\n",
    "out_train[\"ParcelId\"] = train[\"id_parcel\"]\n",
    "for col in months:\n",
    "    out_train[col] = train_pred #+ 0.02 #IMPORTANT POINT: I add a constant to train prediction\n",
    "\n",
    "\n",
    "print(\"Read the missing\")\n",
    "miss = store[\"miss\"]\n",
    "\n",
    "med = train.logerror.median()\n",
    "for col in months:\n",
    "    miss[col] = med\n",
    "    \n",
    "miss = miss[[\"id_parcel\"]+months]\n",
    "miss.columns = [\"ParcelId\"] + months\n",
    "\n",
    "out = pd.concat([out, out_train, miss], axis=0)\n",
    "\n",
    "from datetime import datetime\n",
    "out.to_csv('xgb.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False, float_format='%.4f')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
